{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdcc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.formula.api import ols\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f1c32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4b/ccmglf5s62j_vb3p4cf074_80000gn/T/ipykernel_37464/1153554593.py:8: DtypeWarning: Columns (1,4,7,8,9,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_data(path):\n",
    "    path_new = Path(path)\n",
    "    if not path_new.exists():\n",
    "        raise FileNotFoundError(f\"CSV file not found: {path_new.resolve()}\")\n",
    "    return pd.read_csv(path)\n",
    "df = load_data(\"/Users/adidrawshey/Desktop/final_project/final_project/HD_dataset.CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76d9159",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#sys.path.append(str(Path(\"..\").resolve()))\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_cleaning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     select_columns,\n\u001b[32m      8\u001b[39m     strip_spaces_columns,\n\u001b[32m      9\u001b[39m     normalize_case_columns,\n\u001b[32m     10\u001b[39m     gene_filter,\n\u001b[32m     11\u001b[39m     convert_numeric_columns,\n\u001b[32m     12\u001b[39m     drop_missing_required,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m columns_list = [\u001b[33m\"\u001b[39m\u001b[33mPatient_ID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGene/Factor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDisease_Stage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBrain_Volume_Loss\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAge\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSex\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m sub_df = select_columns(df, columns_list)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from src.data_cleaning import (\n",
    "    select_columns,\n",
    "    strip_spaces_columns,\n",
    "    normalize_case_columns,\n",
    "    gene_filter,\n",
    "    convert_numeric_columns,\n",
    "    drop_missing_required,\n",
    ")\n",
    "\n",
    "columns_list = [\"Patient_ID\", \"Gene/Factor\", \"Disease_Stage\", \"Brain_Volume_Loss\", \"Age\", \"Sex\"]\n",
    "\n",
    "sub_df = select_columns(df, columns_list)\n",
    "sub_df = strip_spaces_columns(sub_df, columns=[\"Gene/Factor\", \"Disease_Stage\", \"Sex\"])\n",
    "sub_df = gene_filter(sub_df, \"Gene/Factor\", values_list=[\"MLH1\", \"MSH3\", \"HTT (Somatic Expansion)\"])\n",
    "sub_df = normalize_case_columns(sub_df, columns=[\"Gene/Factor\", \"Disease_Stage\", \"Sex\"])\n",
    "sub_df = convert_numeric_columns(sub_df, columns=[\"Brain_Volume_Loss\", \"Age\"])\n",
    "sub_df = drop_missing_required(sub_df, columns_list)\n",
    "\n",
    "print(sub_df.shape)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce284da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    Removes duplicated subject IDs.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    keep=\"first\" keeps one row per subject.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m    keep=False removes all repeated subjects entirely.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.drop_duplicates(subset=[id_col], keep=keep)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m dup_df = check_independence_duplicates(\u001b[43msub_df\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mPatient_ID\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(dup_df)\n",
      "\u001b[31mNameError\u001b[39m: name 'sub_df' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "def check_independence_duplicates(df, id_col):\n",
    "    if id_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{id_col}' not found. If you don’t have IDs, independence is judged by study design.\")\n",
    "    dup_ids = df[df.duplicated(subset=[id_col], keep=False)].sort_values(id_col)\n",
    "    return dup_ids  # empty = good sign (no repeats)\n",
    "\n",
    "#New function\n",
    "def drop_duplicate_subjects(df, id_col, keep=\"first\"):\n",
    "    \"\"\"\n",
    "    Removes duplicated subject IDs.\n",
    "    keep=\"first\" keeps one row per subject.\n",
    "    keep=False removes all repeated subjects entirely.\n",
    "    \"\"\"\n",
    "    return df.drop_duplicates(subset=[id_col], keep=keep)\n",
    "dup_df = check_independence_duplicates(sub_df, \"Patient_ID\")\n",
    "\n",
    "print(dup_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_linearity_age_dv(df, dv=\"Brain_Volume_Loss\", cov=\"Age\", show_plot=True, kind=\"scatter\"):\n",
    "    \"\"\"\n",
    "    Checks linearity between a covariate and dependent variable.\n",
    "    'kind' can be \"scatter\" or \"hexbin\" for high-density data.\n",
    "    \"\"\"\n",
    "    # numeric arrays\n",
    "    x = df[cov].astype(float).values\n",
    "    y = df[dv].astype(float).values\n",
    "\n",
    "    # Pearson correlation\n",
    "    r, p = pearsonr(x, y)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        \n",
    "        if kind == \"scatter\":\n",
    "            # s=1 makes dots tiny; alpha adds transparency to show density\n",
    "            plt.scatter(x, y, s=1, alpha=0.3, edgecolors='none', color='steelblue')\n",
    "        elif kind == \"hexbin\":\n",
    "            # Great for 10k+ points to see where the bulk of data lies\n",
    "            hb = plt.hexbin(x, y, gridsize=50, cmap='BuPu', mincnt=1)\n",
    "            plt.colorbar(hb, label='Count')\n",
    "\n",
    "        # best-fit line\n",
    "        m, b = np.polyfit(x, y, 1)\n",
    "        x_line = np.linspace(x.min(), x.max(), 200)\n",
    "        plt.plot(x_line, m * x_line + b, color='darkorange', linewidth=2, label='Best Fit')\n",
    "\n",
    "        plt.xlabel(cov)\n",
    "        plt.ylabel(dv)\n",
    "        plt.title(f\"Linearity: {cov} vs {dv}\\n(r={r:.2f}, p={p:.3g})\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return {\"pearson_r\": float(r), \"p_value\": float(p)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_quadratic_ancova(df, dv, iv, covariate):\n",
    "    \"\"\"\n",
    "    Fits an ANCOVA model with a quadratic covariate:\n",
    "    DV ~ IV + covariate + covariate^2\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Center the covariate (VERY important for stability)\n",
    "    cov_c = f\"{covariate}_c\"\n",
    "    cov_c_sq = f\"{covariate}_c_sq\"\n",
    "\n",
    "    df[cov_c] = df[covariate] - df[covariate].mean()\n",
    "    df[cov_c_sq] = df[cov_c] ** 2\n",
    "\n",
    "    formula = f\"{dv} ~ C({iv}) + {cov_c} + {cov_c_sq}\"\n",
    "\n",
    "    model = ols(formula, data=df).fit()\n",
    "\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "    return model, anova_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def check_homogeneity_of_slopes(df,DV,IV,Covariate):\n",
    "    model = ols(\n",
    "        f\"{DV} ~ C({IV}) * {Covariate}\",\n",
    "        data=df\n",
    "    ).fit()\n",
    "\n",
    "    table = sm.stats.anova_lm(model, typ=2)\n",
    "    # Key row to check: C(Q('disease stage')):Q('age')\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levene_two_way_anova(df, dv, factor1, factor2, center='median'):\n",
    "    groups = [\n",
    "        sub_df[dv].dropna().values\n",
    "        for _, sub_df in df.groupby([factor1, factor2])\n",
    "        if len(sub_df) > 1\n",
    "    ]\n",
    "\n",
    "    stat, p = levene(*groups, center=center)\n",
    "    return stat, p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538825ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import levene\n",
    "def validate_ancova_for_levene(df, dv, iv, covariate):\n",
    "    \"\"\"\n",
    "    Validate data for Levene's test in ANCOVA.\n",
    "    Raises ValueError if assumptions for the test are violated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_clean : pandas.DataFrame\n",
    "        Cleaned dataframe (rows with NaNs dropped)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Column existence ----\n",
    "    required_cols = {dv, iv, covariate}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # ---- Drop NaNs ----\n",
    "    df_clean = df[[dv, iv, covariate]].dropna()\n",
    "\n",
    "    if len(df_clean) < 3:\n",
    "        raise ValueError(\"Not enough observations after dropping missing values.\")\n",
    "\n",
    "    # ---- IV checks ----\n",
    "    n_levels = df_clean[iv].nunique()\n",
    "    if n_levels < 2:\n",
    "        raise ValueError(\n",
    "            f\"Levene's test requires at least 2 levels in '{iv}'. \"\n",
    "            f\"Found {n_levels}.\"\n",
    "        )\n",
    "\n",
    "    group_sizes = df_clean[iv].value_counts()\n",
    "    if (group_sizes < 2).any():\n",
    "        bad_levels = group_sizes[group_sizes < 2].index.tolist()\n",
    "        raise ValueError(\n",
    "            f\"Each level of '{iv}' must have at least 2 observations. \"\n",
    "            f\"Problematic levels: {bad_levels}\"\n",
    "        )\n",
    "    # ---- Covariate checks ----\n",
    "    if df_clean[covariate].nunique() < 2:\n",
    "        raise ValueError(\n",
    "            f\"Covariate '{covariate}' has no variability (constant). \"\n",
    "            \"ANCOVA cannot be fitted.\"\n",
    "        )\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean levene_ancova using the validator\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import levene\n",
    "def levene_ancova(df, dv, iv, covariate, center='median'):\n",
    "    \"\"\"\n",
    "    Levene's test for ANCOVA using model residuals.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input\n",
    "    df_clean = validate_ancova_for_levene(df, dv, iv, covariate)\n",
    "\n",
    "    # Fit ANCOVA model\n",
    "    model = smf.ols(\n",
    "        f\"{dv} ~ C({iv}) + {covariate}\",\n",
    "        data=df_clean\n",
    "    ).fit()\n",
    "\n",
    "    df_clean = df_clean.copy()\n",
    "    df_clean[\"_residuals\"] = model.resid\n",
    "\n",
    "    if df_clean[\"_residuals\"].isna().any():\n",
    "        raise ValueError(\n",
    "            \"Residuals contain NaN values. \"\n",
    "            \"Check model specification or input data.\"\n",
    "        )\n",
    "\n",
    "    # Levene on residuals\n",
    "    groups = [\n",
    "        df_clean.loc[df_clean[iv] == level, \"_residuals\"].values\n",
    "        for level in df_clean[iv].unique()\n",
    "    ]\n",
    "\n",
    "    stat, p = levene(*groups, center=center)\n",
    "    return stat, p\n",
    "# Validation function for Two-Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_two_way_anova_for_levene(df, dv, factor1, factor2):\n",
    "    \"\"\"\n",
    "    Validate data for Levene's test in two-way ANOVA.\n",
    "    Raises ValueError if assumptions for the test are violated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_clean : pandas.DataFrame\n",
    "        Cleaned dataframe (rows with NaNs dropped)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Column existence ----\n",
    "    required_cols = {dv, factor1, factor2}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # ---- Drop NaNs ----\n",
    "    df_clean = df[[dv, factor1, factor2]].dropna()\n",
    "\n",
    "    if len(df_clean) < 4:\n",
    "        raise ValueError(\n",
    "            \"Not enough observations after dropping missing values \"\n",
    "            \"for two-way ANOVA.\"\n",
    "        )\n",
    "\n",
    "    # ---- Factor level checks ----\n",
    "    for factor in (factor1, factor2):\n",
    "        n_levels = df_clean[factor].nunique()\n",
    "        if n_levels < 2:\n",
    "            raise ValueError(\n",
    "                f\"Factor '{factor}' must have at least 2 levels. \"\n",
    "                f\"Found {n_levels}.\"\n",
    "            )\n",
    "\n",
    "    # ---- Cell size checks (factor1 × factor2) ----\n",
    "    cell_sizes = df_clean.groupby([factor1, factor2]).size()\n",
    "    if (cell_sizes < 2).any():\n",
    "        bad_cells = cell_sizes[cell_sizes < 2].index.tolist()\n",
    "        raise ValueError(\n",
    "            \"Each factor1 × factor2 cell must contain at least \"\n",
    "            \"2 observations for Levene's test.\\n\"\n",
    "            f\"Problematic cells: {bad_cells}\"\n",
    "        )\n",
    "\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean levene_two_way_anova using the validator\n",
    "from scipy.stats import levene\n",
    "\n",
    "def levene_two_way_anova(df, dv, factor1, factor2, center='median'):\n",
    "    \"\"\"\n",
    "    Levene's test for two-way ANOVA.\n",
    "    Tests equality of variances across all factor1 × factor2 cells.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input\n",
    "    df_clean = validate_two_way_anova_for_levene(\n",
    "        df, dv, factor1, factor2\n",
    "    )\n",
    "\n",
    "    # Levene across all cells\n",
    "    groups = [\n",
    "        sub_df[dv].values\n",
    "        for _, sub_df in df_clean.groupby([factor1, factor2])\n",
    "    ]\n",
    "\n",
    "    stat, p = levene(*groups, center=center)\n",
    "    return stat, p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normality_of_residuals_visual(df,DV,IV,Covariate):\n",
    "    model = ols(\n",
    "        f\"{DV} ~ C({IV}) * {Covariate}\",\n",
    "        data=df\n",
    "    ).fit()\n",
    "\n",
    "    resid = model.resid.dropna()\n",
    "\n",
    "    # Histogram\n",
    "    plt.figure()\n",
    "    plt.hist(resid, bins=30)\n",
    "    plt.title(\"Residuals Histogram\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Q-Q Plot\n",
    "    plt.figure()\n",
    "    sm.qqplot(resid, line=\"45\")\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "    return {\"n_resid\": int(resid.shape[0])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_transform(\n",
    "    df,\n",
    "    column,\n",
    "    new_column=None,\n",
    "    offset=\"auto\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Log-transform a column safely.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "    column : str\n",
    "        Column to transform\n",
    "    new_column : str or None\n",
    "        Name of transformed column (default: log_<column>)\n",
    "    offset : \"auto\" or float\n",
    "        Value added to make data positive before log\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_out : pandas.DataFrame\n",
    "    offset_used : float\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    x = df_out[column].astype(float)\n",
    "\n",
    "    if offset == \"auto\":\n",
    "        min_val = x.min()\n",
    "        offset_used = abs(min_val) + 1 if min_val <= 0 else 0\n",
    "    else:\n",
    "        offset_used = float(offset)\n",
    "\n",
    "    transformed = np.log(x + offset_used)\n",
    "\n",
    "    if new_column is None:\n",
    "        new_column = f\"log_{column}\"\n",
    "    df_out[new_column] = transformed\n",
    "\n",
    "    return df_out, offset_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84adc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_column(df, col, inplace=False):\n",
    "    \"\"\"\n",
    "    Squares the values of a column.\n",
    "    If inplace=False, returns a new DataFrame.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "        \n",
    "    df[col] = df[col] ** 2\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44727c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vif(df):  #checks multicollinearity, means that two or more predictors in the ANCOVA model are highly correlated with each other. \n",
    "    # Build design matrix like the model would \n",
    "    X = pd.get_dummies(df[[\"disease_stage\", \"age\", \"gender\"]], drop_first=True) #convert CV into dummy variables so they can be used in regression.\n",
    "\n",
    "        #We calculate variance inflation factor(VIF) for each predictor.\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    vifs = []\n",
    "    cols = X.columns.tolist()\n",
    "    X_vals = X.values.astype(float)\n",
    "\n",
    "    for i, col in enumerate(cols):\n",
    "        vif_val = variance_inflation_factor(X_vals, i)\n",
    "        vifs.append({\"feature\": col, \"vif\": float(vif_val)})\n",
    "\n",
    "    return pd.DataFrame(vifs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
